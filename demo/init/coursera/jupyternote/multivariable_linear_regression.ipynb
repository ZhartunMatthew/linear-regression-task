{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    input_file = open(path)\n",
    "    output_data = []\n",
    "    for line in input_file:\n",
    "        one_row = ['1']\n",
    "        one_row += line[:-2].split(',')\n",
    "        output_data.append([float(x) for x in one_row])\n",
    "\n",
    "    print('Input data info.')\n",
    "    print('\\tVariable amount: %d, training set size %d' % (len(output_data[0]) - 1, len(output_data)))\n",
    "    return np.array(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_convergence(data):\n",
    "    convergence_plot_data = np.array(data)\n",
    "    convergence_plot = plt.figure().add_subplot(111)\n",
    "    plt.xlabel('Iteration, N')\n",
    "    plt.ylabel('Cost function, J')\n",
    "    plt.title('Cost function convergence')\n",
    "    convergence_plot.plot(convergence_plot_data[:, 0], convergence_plot_data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_value(x, theta):\n",
    "    return sum(np.array(x) * np.array(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_theta(a, b):\n",
    "    delta = 0.000001\n",
    "    for i in range(len(a)):\n",
    "        if abs(a[i] - b[i]) > delta:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_regression_plot(data, hypothesis=None, labels=None):\n",
    "    x_data = data[:, 1]\n",
    "    y_data = data[:, 2]\n",
    "    plot_regression = plt.figure().add_subplot(111)\n",
    "    plot_regression.scatter(x_data, y_data, s=10)\n",
    "\n",
    "    if labels is not None:\n",
    "        plt.xlabel(labels[0])\n",
    "        plt.ylabel(labels[1])\n",
    "        plt.title(labels[2])\n",
    "\n",
    "    if hypothesis is not None:\n",
    "        plot_regression.plot(x_data, hypothesis, color='red')\n",
    "\n",
    "\n",
    "def display_regression_coefficients(theta):\n",
    "    print('\\nCalculated regression coefficients:')\n",
    "    [print('\\tTheta[%d]: %f' % (i, theta[i])) for i in range(len(theta))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(data, theta, hypothesis=None, labels=None):\n",
    "    display_regression_coefficients(theta)\n",
    "    if len(data[0]) == 3:\n",
    "        display_regression_plot(data, hypothesis, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all_plots():\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(x_data, y_data, theta):\n",
    "    coefficient = 0.5 / len(x_data)\n",
    "    hyp_data = np.array([hyp_value(x, theta) for x in x_data])\n",
    "    return sum(((hyp_data - y_data) ** 2)) * coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_partial_derivative(x_data, y_data, theta, j):\n",
    "    result = 0.0\n",
    "    for i in range(len(x_data)):\n",
    "        result += (hyp_value(x_data[i], theta) - y_data[i]) * x_data[i][j]\n",
    "\n",
    "    return result / len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(x_data, y_data, theta, alpha):\n",
    "    return [theta[i] - alpha * compute_partial_derivative(x_data, y_data, theta, i) for i in range(len(theta))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x_data, y_data, theta, alpha, iterations):\n",
    "    prev_theta = list(theta)\n",
    "    convergence_plot_data = []\n",
    "    for i in range(1, iterations):\n",
    "        theta = gradient_step(x_data, y_data, theta, alpha)\n",
    "        if compare_theta(prev_theta, theta):\n",
    "            print('\\tEstimated iterations: %d\\n' % i)\n",
    "            break\n",
    "\n",
    "        prev_theta = list(theta)\n",
    "        convergence_plot_data.append([i, compute_cost_function(x_data, y_data, theta)])\n",
    "\n",
    "    if convergence_plot_data:\n",
    "        display_convergence(convergence_plot_data)\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_all(data):\n",
    "    for i in range(1, len(data[0])):\n",
    "        data[:, i] = scale_normalize(data[:, i])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def scale_normalize(lst):\n",
    "    return lst / np.mean(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as lin\n",
    "\n",
    "\n",
    "def compute_normal_equation(x_data, y_data):\n",
    "    # xT - transposed x-matrix\n",
    "    x_transpose = x_data.transpose()\n",
    "    # X' * X\n",
    "    x_mul = x_transpose @ x_data\n",
    "    # (X' * X) ** (-1)\n",
    "    x_mul_inv = lin.inv(x_mul)\n",
    "    # ((X' * X) ** (-1))' * X'\n",
    "    x_mul_on_transpose = x_mul_inv @ x_transpose\n",
    "    # ((X' * X) ** (-1))' * X' * y\n",
    "    return x_mul_on_transpose @ y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data info.\n\tVariable amount: 3, training set size 47\nGradient params.\n\tRandomly generated theta vector: [0 - 0.0711] [1 - 0.6360] [2 - 0.2514] \n\tAlpha:                           0.100000\n\tMax iteration count:             10000\n\nGradient descent started...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEstimated iterations: 6987\n\n\nCalculated regression coefficients:\n\tTheta[0]: 8959.841485\n\tTheta[1]: 27851.696104\n\tTheta[2]: -2770.346107\nInput data info.\n\tVariable amount: 3, training set size 47\n\nNormal equation started...\n\nCalculated regression coefficients:\n\tTheta[0]: 8959.841796\n\tTheta[1]: 27851.696210\n\tTheta[2]: -2770.346516\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13b81e22dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def compute_linear_regression():\n",
    "    data = load_data('demo/init/data/ex1data2.txt')\n",
    "\n",
    "    # getting separated x and y data for descent and cost function\n",
    "    # and normalization x-data\n",
    "    x_data = normalize_all(data[:, :-1])\n",
    "    y_data = data[:,  -1]\n",
    "\n",
    "    # generating random start theta (or coefficients on linear regression)\n",
    "    theta = [random.uniform(0, 1) for i in range(len(x_data[0]))]\n",
    "\n",
    "    # descent speed\n",
    "    # decrease alpha if:\n",
    "    #   - script threw an overflow warning\n",
    "    #   - J-function on plot increase instead of decrease\n",
    "    # increase alpha if:\n",
    "    #   - J-function on plot decrease too slow or looks like linear function\n",
    "    #   - descent finished after first iteration or very fast\n",
    "    #   - descent finished all iterations, but regression looks even not close to true\n",
    "    # for ex1data1.txt alpha = 0.01 is perfect choice (if data not normalize)\n",
    "    # for ex1data2.txt alpha = 0.01 if too big, 10**(-10) - 10**(-7) is better choice (is not normalize)\n",
    "    # if data normalized alpha = 0.1 is perfect choice for both examples\n",
    "    alpha = pow(10, -1)\n",
    "\n",
    "    # if cost function not converged, descent will stop after all iterations\n",
    "    iterations = 10000\n",
    "\n",
    "    # computing gradient descent\n",
    "    print('Gradient params.')\n",
    "    print('\\tRandomly generated theta vector: ', end='')\n",
    "    [print('[%d - %.4f]' % (i, theta[i]), end=' ') for i in range(len(theta))]\n",
    "    print('\\n\\tAlpha:                           %f' % alpha)\n",
    "    print('\\tMax iteration count:             %d' % iterations)\n",
    "    print('\\nGradient descent started...')\n",
    "\n",
    "    # computing gradient descent\n",
    "    theta = gradient_descent(x_data, y_data, theta, alpha, iterations)\n",
    "\n",
    "    # displaying results\n",
    "    # if regression with one variable plot will be displayed\n",
    "    prediction = [hyp_value(x, theta) for x in x_data]\n",
    "    labels = ['X-axis', 'Y-axis', 'Title']\n",
    "    display_results(data, theta, prediction, labels)\n",
    "\n",
    "\n",
    "def compute_regression_normal_equation():\n",
    "    data = load_data('demo/init/data/ex1data2.txt')\n",
    "    # data normalization for normal equation isn't necessary\n",
    "    # this only for comparing results of gradient descent and normal equation\n",
    "    x_data = normalize_all(data[:, :-1])\n",
    "    y_data = data[:,  -1]\n",
    "\n",
    "    print('\\nNormal equation started...')\n",
    "    theta = compute_normal_equation(x_data, y_data)\n",
    "    prediction = [hyp_value(x, theta) for x in x_data]\n",
    "    labels = ['X-axis', 'Y-axis', 'Normal equation']\n",
    "    display_results(data, theta, prediction, labels)\n",
    "\n",
    "\n",
    "compute_linear_regression()\n",
    "compute_regression_normal_equation()\n",
    "display_all_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}