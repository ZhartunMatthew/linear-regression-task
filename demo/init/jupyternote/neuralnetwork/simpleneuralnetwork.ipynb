{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    input_file = open(path)\n",
    "    output_data = []\n",
    "    for line in input_file:\n",
    "        one_row = line.split(',')\n",
    "        output_data.append([float(x) for x in one_row])\n",
    "\n",
    "    print('Input data info.')\n",
    "    print('\\tFeatures amount: %d, set size %d' % (len(output_data[0]), len(output_data)))\n",
    "    return np.array(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(output_vector):\n",
    "    index_of_max = np.argmax(np.roll(output_vector[1:], 1))\n",
    "    return index_of_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of all pairs theta[i]*input[i] in one neuron\n",
    "# z(i) = sum(theta * x) = theta' * x\n",
    "def unit_summarization(theta, input_vec):\n",
    "    return (np.array(theta) * np.array(input_vec)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the activation function of neuron\n",
    "# a(i) = g(z(i))\n",
    "def unit_activation(theta, input_vec):\n",
    "    sum_result = unit_summarization(theta, input_vec)\n",
    "    return sigmoid(sum_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the activation vector of whole network layer\n",
    "# A = g(z(i)) for all units on layer\n",
    "# [1] - additional neuron (bias)\n",
    "def forward_propagation_step(theta_matrix, input_vec):\n",
    "    activations_vector = [unit_activation(theta_matrix[i], input_vec) for i in range(len(theta_matrix))] \n",
    "    return np.concatenate(([1], activations_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(theta_one, theta_two, input_vec):\n",
    "    # computing input level activation vector\n",
    "    activation_vector_input_layer = np.concatenate(([1], input_vec))\n",
    "    # computing second level activation vector\n",
    "    activation_vector_hidden_layer = forward_propagation_step(theta_one, activation_vector_input_layer)\n",
    "    # computing output level activation vector\n",
    "    activation_vector_output_layer = forward_propagation_step(theta_two, activation_vector_hidden_layer)\n",
    "    return [activation_vector_hidden_layer, activation_vector_output_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regularize_coefficient(lm, m, theta):\n",
    "    coefficient = lm / (2.0 * m)\n",
    "    a = np.array(theta[0]) * np.array(theta[0])\n",
    "    b = np.array(theta[1]) * np.array(theta[1])\n",
    "    return (a.sum() + b.sum()) * coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(i, k):\n",
    "    offset = 500\n",
    "    return 1.0 if k * offset <= i < k * offset + offset else 0.0\n",
    "\n",
    "\n",
    "def get_output_vector(num):\n",
    "    out_vec = [0.0] * 10\n",
    "    out_vec[int(num - 1)] = 1.0\n",
    "    return [1.0] + out_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(x_data, theta, lm):\n",
    "    temp_sum = 0.0\n",
    "    coefficient = -1 / len(x_data)\n",
    "    fake_zero = 10 ** (-9)\n",
    "    \n",
    "    regularize_coefficient = compute_regularize_coefficient(lm, len(x_data), theta)\n",
    "    for k in range(10):\n",
    "        for i in range(len(x_data)):\n",
    "            hyp_vector = forward_propagation(theta[0], theta[1], x_data[i])[1]\n",
    "            hyp = np.roll(hyp_vector[1:], 1)[k]\n",
    "            temp_sum += get_y(i, k) * np.log(hyp if hyp != 0.0 else fake_zero) \\\n",
    "                + (1.0 - get_y(i, k)) * np.log(1 - hyp if 1 - hyp != 0.0 else fake_zero)\n",
    " \n",
    "    return temp_sum * coefficient + regularize_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unit_error(theta, delta):\n",
    "    return (np.array(theta) * np.array(delta)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all layers except last one\n",
    "def compute_layer_error(layer_delta, theta):\n",
    "    layer_delta = layer_delta[1:]\n",
    "    temp_theta = np.transpose(theta)\n",
    "    return [compute_unit_error(temp_theta[i], layer_delta) for i in range(len(temp_theta))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_delta(layer_error, layer_activation):\n",
    "    return layer_error * layer_activation * (1.0 - layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_theta_correction(prev_layer_activation, delta, rate):\n",
    "    return [prev_layer_activation[i] * delta * rate for i in range(len(prev_layer_activation))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_theta_correction(prev_layer_activation, layer_delta, lr, additional_neuron=False):\n",
    "    layer_delta = layer_delta[1:]\n",
    "    prev_layer_activation = np.concatenate(([[1.0], prev_layer_activation])) if additional_neuron else prev_layer_activation\n",
    "    return [unit_theta_correction(prev_layer_activation, layer_delta[i], lr) for i in range(len(layer_delta))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(x_data, theta, learning_rate):\n",
    "    theta_one = np.array(theta[0])\n",
    "    theta_two = np.array(theta[1])\n",
    "\n",
    "    for i in range(len(x_data)):\n",
    "        # running forward propagation for getting hypothesis  \n",
    "        layer_activations = forward_propagation(theta_one, theta_two, x_data[i])\n",
    "\n",
    "        # getting outputs of hidden and output layer\n",
    "        output_layer_activation = layer_activations[1]\n",
    "        hidden_layer_activation = layer_activations[0]\n",
    "\n",
    "        ############### output layer section ############\n",
    "        # computing output layer error, it's different for all other layers \n",
    "        output_layer_error = output_layer_activation - get_output_vector(i / 500)\n",
    "        # computing deltas, it will be used for computing new theta (new weights)\n",
    "        # delta = error * sigmoid(x)dx\n",
    "        output_layer_delta = compute_layer_delta(output_layer_error, output_layer_activation)\n",
    "        # correcting theta\n",
    "        theta_two -= np.array(layer_theta_correction(hidden_layer_activation, output_layer_delta, learning_rate))\n",
    "\n",
    "        # ############### hidden layer section ############\n",
    "        hidden_layer_error = compute_layer_error(output_layer_delta, theta_two)\n",
    "        hidden_layer_delta = compute_layer_delta(hidden_layer_error, hidden_layer_activation)\n",
    "        theta_one -= np.array(layer_theta_correction(x_data[i], hidden_layer_delta, learning_rate, True))\n",
    "        \n",
    "    return [theta_one, theta_two]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data info.\n\tFeatures amount: 400, set size 5000\nInput data info.\n\tFeatures amount: 1, set size 5000\nInput data info.\n\tFeatures amount: 401, set size 25\nInput data info.\n\tFeatures amount: 26, set size 10\n"
     ]
    }
   ],
   "source": [
    "loaded_data_x = load_data(abspath('demo/init/coursera/neuralnetwork/data/data_x.csv'))\n",
    "loaded_data_y = load_data(abspath('demo/init/coursera/neuralnetwork/data/data_y.csv'))\n",
    "loaded_theta_one = load_data(abspath('demo/init/coursera/neuralnetwork/data/theta1.csv'))\n",
    "loaded_theta_two = load_data(abspath('demo/init/coursera/neuralnetwork/data/theta2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_network(expected_theta, actual_theta, x_data):\n",
    "    expected = forward_propagation(expected_theta[0], expected_theta[1], x_data)[1]\n",
    "    actual = forward_propagation(actual_theta[0], actual_theta[1], x_data)[1]\n",
    "    print('Expected: %d  Actual: %d' % (show_prediction(expected), show_prediction(actual)))\n",
    "    print('Output: ', actual[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 2  Actual: 7\nOutput:  [ 0.51995899  0.52146164  0.51760932  0.51781576  0.51865624  0.51962614\n  0.52350478  0.51934481  0.5173121   0.52053005]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-b4043970d317>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcomputed_thetas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mcool_thetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-78-b4043970d317>\u001b[0m in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mcheck_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloaded_theta_one\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_theta_two\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomputed_thetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_data_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1001\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mcomputed_thetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mback_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_data_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomputed_thetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcomputed_thetas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-74-e1c68e5ea1e9>\u001b[0m in \u001b[0;36mback_propagation\u001b[1;34m(x_data, theta, learning_rate)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mhidden_layer_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_layer_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_layer_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_two\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mhidden_layer_delta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_layer_delta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layer_activation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mtheta_one\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_theta_correction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layer_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtheta_one\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_two\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def run():\n",
    "    features_amount = 400\n",
    "    hidden_layer_neuron_amount = 25\n",
    "    hidden_layer_inputs_per_neuron = features_amount + 1\n",
    "    output_layer_neuron_amount = 10\n",
    "    output_layer_inputs_per_neuron = hidden_layer_neuron_amount + 1\n",
    "    learning_rate = 0.1\n",
    "    epochs = 100\n",
    "    computed_thetas = [[], []]\n",
    "    # initializing thetas (weights of each connection between neurons)\n",
    "    # weights between input and hidden layer\n",
    "    computed_thetas[0] = np.random.rand(hidden_layer_neuron_amount, hidden_layer_inputs_per_neuron) / 100\n",
    "    # weights between hidden and output layer\n",
    "    computed_thetas[1] = np.random.rand(output_layer_neuron_amount, output_layer_inputs_per_neuron) / 100\n",
    "\n",
    "    for i in range(epochs):\n",
    "        check_network([loaded_theta_one, loaded_theta_two], computed_thetas, loaded_data_x[1001])       \n",
    "        computed_thetas = back_propagation(loaded_data_x, computed_thetas, learning_rate) \n",
    "            \n",
    "    return computed_thetas\n",
    "\n",
    "cool_thetas = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
