{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    input_file = open(path)\n",
    "    output_data = []\n",
    "    for line in input_file:\n",
    "        one_row = line.split(',')\n",
    "        output_data.append([float(x) for x in one_row])\n",
    "\n",
    "    print('Input data info.')\n",
    "    print('\\tFeatures amount: %d, set size %d' % (len(output_data[0]), len(output_data)))\n",
    "    return np.array(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(output_vector):\n",
    "    index_of_max = np.argmax(np.roll(output_vector[1:], 1))\n",
    "    return index_of_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of all pairs theta[i]*input[i] in one neuron\n",
    "# z(i) = sum(theta * x) = theta' * x\n",
    "def unit_summarization(theta, input_vec):\n",
    "    return (np.array(theta) * np.array(input_vec)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the activation function of neuron\n",
    "# a(i) = g(z(i))\n",
    "def unit_activation(theta, input_vec):\n",
    "    sum_result = unit_summarization(theta, input_vec)\n",
    "    return sigmoid(sum_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the activation vector of whole network layer\n",
    "# A = g(z(i)) for all units on layer\n",
    "# [1] - additional neuron (bias)\n",
    "def forward_propagation_step(theta_matrix, input_vec):\n",
    "    activations_vector = [unit_activation(theta_matrix[i], input_vec) for i in range(len(theta_matrix))] \n",
    "    return np.concatenate(([1.0], activations_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(theta_one, theta_two, input_vec):\n",
    "    # computing input level activation vector\n",
    "    activation_vector_input_layer = np.concatenate(([1.0], input_vec))\n",
    "    # computing second level activation vector\n",
    "    activation_vector_hidden_layer = forward_propagation_step(theta_one, activation_vector_input_layer)\n",
    "    # computing output level activation vector\n",
    "    activation_vector_output_layer = forward_propagation_step(theta_two, activation_vector_hidden_layer)\n",
    "    return [activation_vector_hidden_layer, activation_vector_output_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regularize_coefficient(lm, m, theta):\n",
    "    coefficient = lm / (2.0 * m)\n",
    "    a = np.array(theta[0]) * np.array(theta[0])\n",
    "    b = np.array(theta[1]) * np.array(theta[1])\n",
    "    return (a.sum() + b.sum()) * coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(i, k):\n",
    "    offset = 500\n",
    "    return 1.0 if k * offset <= i < k * offset + offset else 0.0\n",
    "\n",
    "\n",
    "def get_output_vector(num):\n",
    "    out_vec = [0.0] * 10\n",
    "    out_vec[int(num - 1)] = 1.0\n",
    "    return [1.0] + out_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(x_data, theta, lm):\n",
    "    temp_sum = 0.0\n",
    "    coefficient = -1 / len(x_data)\n",
    "    fake_zero = 10 ** (-9)\n",
    "    \n",
    "    regularize_coefficient = compute_regularize_coefficient(lm, len(x_data), theta)\n",
    "    for k in range(10):\n",
    "        for i in range(len(x_data)):\n",
    "            hyp_vector = forward_propagation(theta[0], theta[1], x_data[i])[1]\n",
    "            hyp = np.roll(hyp_vector[1:], 1)[k]\n",
    "            temp_sum += get_y(i, k) * np.log(hyp if hyp != 0.0 else fake_zero) \\\n",
    "                + (1.0 - get_y(i, k)) * np.log(1 - hyp if 1 - hyp != 0.0 else fake_zero)\n",
    " \n",
    "    return temp_sum * coefficient + regularize_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all layers except last one\n",
    "def compute_layer_error(layer_delta, theta):\n",
    "    return theta.transpose() @ layer_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_delta(layer_error, layer_activation):\n",
    "    layer_error = np.array(layer_error)\n",
    "    return (layer_error * layer_activation * (1.0 - layer_activation))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_theta_correction(prev_layer_activation, delta, rate):\n",
    "    return prev_layer_activation * delta * rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_theta_correction(prev_layer_activation, layer_delta, lr, add_neuron=False):\n",
    "    prev_layer_activation = np.concatenate(([[1.0], prev_layer_activation])) if add_neuron else prev_layer_activation\n",
    "    return [unit_theta_correction(prev_layer_activation, layer_delta[i], lr) for i in range(len(layer_delta))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(x_data, theta, learning_rate):\n",
    "    theta_one = np.array(theta[0])\n",
    "    theta_two = np.array(theta[1])\n",
    "\n",
    "    for i in range(len(x_data)):\n",
    "        # running forward propagation for getting hypothesis  \n",
    "        layer_activations = forward_propagation(theta_one, theta_two, x_data[i])\n",
    "\n",
    "        # getting outputs of hidden and output layer\n",
    "        output_layer_activation = layer_activations[1]\n",
    "        hidden_layer_activation = layer_activations[0]\n",
    "\n",
    "        ############### output layer section ############\n",
    "        # computing output layer error, it's different for all other layers \n",
    "        output_layer_error = output_layer_activation - get_output_vector(i / 500)\n",
    "        # computing deltas, it will be used for computing new theta (new weights)\n",
    "        # delta = error * sigmoid(x)dx\n",
    "        output_layer_delta = compute_layer_delta(output_layer_error, output_layer_activation)\n",
    "        # correcting theta\n",
    "        theta_two -= np.array(layer_theta_correction(hidden_layer_activation, output_layer_delta, learning_rate))\n",
    "\n",
    "        # ############### hidden layer section ############\n",
    "        hidden_layer_error = compute_layer_error(output_layer_delta, theta_two)\n",
    "        hidden_layer_delta = compute_layer_delta(hidden_layer_error, hidden_layer_activation)\n",
    "        theta_one -= np.array(layer_theta_correction(x_data[i], hidden_layer_delta, learning_rate, True))\n",
    "        \n",
    "    return [theta_one, theta_two]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data info.\n\tFeatures amount: 400, set size 5000\nInput data info.\n\tFeatures amount: 1, set size 5000\nInput data info.\n\tFeatures amount: 401, set size 25\nInput data info.\n\tFeatures amount: 26, set size 10\n"
     ]
    }
   ],
   "source": [
    "loaded_data_x = load_data(abspath('demo/init/coursera/neuralnetwork/data/data_x.csv'))\n",
    "loaded_data_y = load_data(abspath('demo/init/coursera/neuralnetwork/data/data_y.csv'))\n",
    "loaded_theta_one = load_data(abspath('demo/init/coursera/neuralnetwork/data/theta1.csv'))\n",
    "loaded_theta_two = load_data(abspath('demo/init/coursera/neuralnetwork/data/theta2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_network(expected_theta, actual_theta, x_data):\n",
    "    expected = forward_propagation(expected_theta[0], expected_theta[1], x_data)[1]\n",
    "    actual = forward_propagation(actual_theta[0], actual_theta[1], x_data)[1]\n",
    "    print('Expected: %d  Actual: %d' % (show_prediction(expected), show_prediction(actual)))\n",
    "    print('Output: ', list(actual[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 0  Actual: 1\nOutput:  [0.99121418609407208, 0.0017610407981747925, 0.0023183744138181484, 8.0462640363083303e-07, 0.013654034012992861, 0.00019043141178000459, 0.0016149095317928922, 0.00025323160865909895, 1.5352015814622589e-05, 0.00063232322633801936]\nExpected: 1  Actual: 1\nOutput:  [0.89831576875075703, 0.069769395290902883, 0.0022844766812163565, 0.00014518608472565954, 0.0010611754391410177, 0.00041597637326421731, 0.0025197711065866918, 0.02447391223359727, 2.5057160909754739e-05, 0.001750608777421648]\nExpected: 2  Actual: 2\nOutput:  [0.00070701932153999735, 0.81152429628204747, 0.0085174343899941853, 8.1520039005223061e-05, 0.00029434527348088394, 8.2184828863994812e-06, 0.00084219220231076316, 0.088068595651955933, 0.027697387733959966, 0.0013551287569565536]\nExpected: 3  Actual: 3\nOutput:  [0.0010984054122966356, 0.0020312749733667366, 0.9850275244216381, 1.8237449320348592e-05, 0.0054097272552923893, 5.9826322871933931e-07, 0.0011441105836785574, 0.0069309986116280311, 0.0016279959933590267, 0.0015189946834026937]\nExpected: 4  Actual: 4\nOutput:  [2.1668213154873916e-05, 0.025039273584647068, 0.0017832301641410306, 0.9800544298960423, 0.0027356687915945348, 0.0015118035830527163, 0.021860605733866677, 0.041189962732378467, 0.00042110520953592308, 0.0056025979756562086]\nExpected: 5  Actual: 5\nOutput:  [0.0003508306046081899, 0.00060271171575472765, 0.071469866538034499, 1.3559155876519548e-07, 0.95684062027609373, 8.6310520120595128e-06, 0.0016282721662082729, 0.0082696394149972784, 0.0047024713306793092, 0.0015092098362120133]\nExpected: 6  Actual: 6\nOutput:  [0.0014955762992514322, 0.020837175726480032, 0.00027365292213487957, 0.0070521743674460038, 0.00030885453802930742, 0.98905597851454474, 1.8789120762018857e-05, 0.01113114582604637, 0.00035647822373859847, 0.0018928359884931953]\nExpected: 7  Actual: 7\nOutput:  [0.00071140780300918644, 3.6142106462855581e-05, 3.793810215303509e-05, 0.0026115412550655956, 0.0017411193243236183, 1.2736843846598478e-05, 0.83017700945349582, 0.00048156081371989796, 0.28560261986565488, 0.0020275783386758876]\nExpected: 8  Actual: 8\nOutput:  [0.012631262977619661, 0.0020613988535762209, 0.0011464876896088406, 0.00010839849277920729, 0.01802486114112558, 2.5628162447352637e-05, 2.3554608120088037e-05, 0.97286799873398377, 0.098307984194176329, 0.0018794982381061594]\nExpected: 9  Actual: 9\nOutput:  [0.0040260715391872157, 3.0675737839688775e-05, 4.5069578667249316e-05, 0.023588841854403745, 0.0016020676397439071, 0.00038420365488539549, 0.016874630764616583, 0.00051559745758869948, 0.98576074967993133, 0.0025444862965013746]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run():\n",
    "    features_amount = 400\n",
    "    hidden_layer_neuron_amount = 25\n",
    "    hidden_layer_inputs_per_neuron = features_amount + 1\n",
    "    output_layer_neuron_amount = 10\n",
    "    output_layer_inputs_per_neuron = hidden_layer_neuron_amount + 1\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    epochs = 200\n",
    "    \n",
    "    computed_thetas = [[], []]\n",
    "    # initializing thetas (weights of each connection between neurons)\n",
    "    # weights between input and hidden layer\n",
    "    computed_thetas[0] = np.random.rand(hidden_layer_neuron_amount, hidden_layer_inputs_per_neuron) / 100\n",
    "    # weights between hidden and output layer\n",
    "    computed_thetas[1] = np.random.rand(output_layer_neuron_amount, output_layer_inputs_per_neuron) / 100\n",
    "\n",
    "    for i in range(epochs):\n",
    "        if i % 10 == 0:\n",
    "            print('Epochs left', epochs - i)\n",
    "        computed_thetas = back_propagation(loaded_data_x, computed_thetas, learning_rate) \n",
    "        \n",
    "    return computed_thetas\n",
    "\n",
    "cool_thetas = run()\n",
    "\n",
    "[check_network([loaded_theta_one, loaded_theta_two], cool_thetas, loaded_data_x[j * 500]) for j in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
