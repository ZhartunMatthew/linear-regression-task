{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    output_data = []\n",
    "    for line in open(path):\n",
    "        one_row = line.split(',')\n",
    "        output_data.append([float(x) for x in one_row])\n",
    "\n",
    "    print('Input data info.')\n",
    "    print('\\tFeatures amount: %d, set size %d' % (len(output_data[0]), len(output_data)))\n",
    "    return np.array(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(output_vector):\n",
    "    index_of_max = np.argmax(np.roll(output_vector[1:], 1))\n",
    "    return index_of_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of all pairs theta[i]*input[i] in one neuron\n",
    "# z(i) = sum(theta * x) = theta' * x\n",
    "def unit_summarization(theta, input_vec):\n",
    "    return np.array(theta) @ np.array(input_vec).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the activation function of neuron\n",
    "# a(i) = g(z(i))\n",
    "def unit_activation(theta, input_vec):\n",
    "    sum_result = unit_summarization(theta, input_vec)\n",
    "    return sigmoid(sum_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the activation vector of whole network layer\n",
    "# A = g(z(i)) for all units on layer\n",
    "# [1] - additional neuron (bias)\n",
    "def forward_propagation_step(theta_matrix, input_vec):\n",
    "    activations_vector = [unit_activation(theta_matrix[i], input_vec) for i in range(len(theta_matrix))] \n",
    "    return np.concatenate(([1.0], activations_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(theta_one, theta_two, input_vec):\n",
    "    # computing input level activation vector, and concatenating additional neuron (bias)\n",
    "    activation_vector_input_layer = np.concatenate(([1.0], input_vec))\n",
    "    # computing second level activation vector\n",
    "    activation_vector_hidden_layer = forward_propagation_step(theta_one, activation_vector_input_layer)\n",
    "    # computing output level activation vector\n",
    "    activation_vector_output_layer = forward_propagation_step(theta_two, activation_vector_hidden_layer)\n",
    "    # return each layer activation, that will be used in back propagation \n",
    "    return [activation_vector_hidden_layer, activation_vector_output_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regularize_coefficient(lm, m, theta):\n",
    "    coefficient = lm / (2.0 * m)\n",
    "    a = np.array(theta[0]) * np.array(theta[0])\n",
    "    b = np.array(theta[1]) * np.array(theta[1])\n",
    "    return (a.sum() + b.sum()) * coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(i, k):\n",
    "    offset = 500\n",
    "    return 1.0 if k * offset <= i < k * offset + offset else 0.0\n",
    "\n",
    "\n",
    "def get_output_vector(num):\n",
    "    out_vec = [0.0] * 10\n",
    "    out_vec[int(num - 1)] = 1.0\n",
    "    return [1.0] + out_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(x_data, theta, lm):\n",
    "    temp_sum = 0.0\n",
    "    coefficient = -1 / len(x_data)\n",
    "    fake_zero = 10 ** (-9)\n",
    "    \n",
    "    regularize_coefficient = compute_regularize_coefficient(lm, len(x_data), theta)\n",
    "    for k in range(10):\n",
    "        for i in range(len(x_data)):\n",
    "            hyp_vector = forward_propagation(theta[0], theta[1], x_data[i])[1]\n",
    "            hyp = np.roll(hyp_vector[1:], 1)[k]\n",
    "            temp_sum += get_y(i, k) * np.log(hyp if hyp != 0.0 else fake_zero) \\\n",
    "                + (1.0 - get_y(i, k)) * np.log(1 - hyp if 1 - hyp != 0.0 else fake_zero)\n",
    " \n",
    "    return temp_sum * coefficient + regularize_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all layers except last one\n",
    "def compute_layer_error(layer_delta, theta):\n",
    "    return theta.transpose() @ layer_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing weights correction layer  \n",
    "def compute_layer_delta(layer_error, layer_activation):\n",
    "    return (np.array(layer_error) * layer_activation * (1.0 - layer_activation))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_theta_correction(prev_layer_activation, delta, rate):\n",
    "    return prev_layer_activation * delta * rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_theta_correction(prev_layer_activation, layer_delta, lr, add_neuron=False):\n",
    "    prev_layer_activation = np.concatenate(([[1.0], prev_layer_activation])) if add_neuron else prev_layer_activation\n",
    "    return [unit_theta_correction(prev_layer_activation, layer_delta[i], lr) for i in range(len(layer_delta))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(x_data, theta, learning_rate):\n",
    "    theta_one = np.array(theta[0])\n",
    "    theta_two = np.array(theta[1])\n",
    "\n",
    "    for i in range(len(x_data)):\n",
    "        # running forward propagation for getting hypothesis  \n",
    "        layer_activations = forward_propagation(theta_one, theta_two, x_data[i])\n",
    "\n",
    "        # getting outputs of hidden and output layer\n",
    "        output_layer_activation = layer_activations[1]\n",
    "        hidden_layer_activation = layer_activations[0]\n",
    "\n",
    "        ############### output layer section ############\n",
    "        # computing output layer error, it's different for all other layers \n",
    "        output_layer_error = output_layer_activation - get_output_vector(i / 500)\n",
    "        # computing deltas, it will be used for computing new theta (new weights)\n",
    "        # delta = error * sigmoid(x)dx\n",
    "        output_layer_delta = compute_layer_delta(output_layer_error, output_layer_activation)\n",
    "        # correcting theta\n",
    "        theta_two -= np.array(layer_theta_correction(hidden_layer_activation, output_layer_delta, learning_rate))\n",
    "\n",
    "        # ############### hidden layer section ############\n",
    "        # computing layer error as unit_theta * layer_delta\n",
    "        hidden_layer_error = compute_layer_error(output_layer_delta, theta_two)\n",
    "        hidden_layer_delta = compute_layer_delta(hidden_layer_error, hidden_layer_activation)\n",
    "        # correcting theta\n",
    "        theta_one -= np.array(layer_theta_correction(x_data[i], hidden_layer_delta, learning_rate, add_neuron=True))\n",
    "        \n",
    "    return [theta_one, theta_two]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data info.\n\tFeatures amount: 400, set size 5000\nInput data info.\n\tFeatures amount: 1, set size 5000\nInput data info.\n\tFeatures amount: 401, set size 25\nInput data info.\n\tFeatures amount: 26, set size 10\n"
     ]
    }
   ],
   "source": [
    "loaded_data_x = load_data(abspath('demo/init/coursera/neuralnetwork/data/data_x.csv'))\n",
    "loaded_data_y = load_data(abspath('demo/init/coursera/neuralnetwork/data/data_y.csv'))\n",
    "loaded_theta_one = load_data(abspath('demo/init/coursera/neuralnetwork/data/theta1.csv'))\n",
    "loaded_theta_two = load_data(abspath('demo/init/coursera/neuralnetwork/data/theta2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_network(expected_theta, actual_theta, input_vec):\n",
    "    expected = forward_propagation(expected_theta[0], expected_theta[1], input_vec)[1]\n",
    "    actual = forward_propagation(actual_theta[0], actual_theta[1], input_vec)[1]\n",
    "    print('Expected: %d  Actual: %d' % (show_prediction(expected), show_prediction(actual)))\n",
    "    print('Output: [', end='')\n",
    "    [print(\"%0.3f\" % f, end=' ') for f in actual[1:]]\n",
    "    print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 0  Actual: 1\nOutput: [0.937 0.004 0.001 0.000 0.005 0.000 0.004 0.000 0.000 0.000 ]\nExpected: 1  Actual: 1\nOutput: [0.902 0.044 0.001 0.000 0.003 0.001 0.007 0.018 0.000 0.003 ]\nExpected: 2  Actual: 2\nOutput: [0.001 0.927 0.002 0.000 0.000 0.000 0.004 0.045 0.005 0.002 ]\nExpected: 3  Actual: 3\nOutput: [0.000 0.001 0.988 0.000 0.069 0.000 0.000 0.029 0.001 0.002 ]\nExpected: 4  Actual: 4\nOutput: [0.000 0.038 0.000 0.954 0.002 0.002 0.034 0.018 0.001 0.006 ]\nExpected: 5  Actual: 5\nOutput: [0.001 0.000 0.039 0.000 0.958 0.000 0.002 0.010 0.005 0.001 ]\nExpected: 6  Actual: 6\nOutput: [0.001 0.017 0.000 0.008 0.000 0.989 0.000 0.001 0.000 0.001 ]\nExpected: 7  Actual: 7\nOutput: [0.001 0.000 0.001 0.008 0.001 0.000 0.745 0.000 0.478 0.002 ]\nExpected: 8  Actual: 8\nOutput: [0.006 0.001 0.002 0.001 0.059 0.000 0.000 0.990 0.117 0.003 ]\nExpected: 9  Actual: 9\nOutput: [0.003 0.000 0.000 0.062 0.001 0.001 0.010 0.001 0.985 0.002 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run():\n",
    "    features_amount = 400\n",
    "    hidden_layer_neuron_amount = 25\n",
    "    hidden_layer_inputs_per_neuron = features_amount + 1\n",
    "    output_layer_neuron_amount = 10\n",
    "    output_layer_inputs_per_neuron = hidden_layer_neuron_amount + 1\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    epochs = 200\n",
    "    \n",
    "    computed_thetas = [[], []]\n",
    "    # initializing thetas (weights of each connection between neurons)\n",
    "    # weights between input and hidden layer\n",
    "    computed_thetas[0] = np.random.rand(hidden_layer_neuron_amount, hidden_layer_inputs_per_neuron) / 100\n",
    "    # weights between hidden and output layer\n",
    "    computed_thetas[1] = np.random.rand(output_layer_neuron_amount, output_layer_inputs_per_neuron) / 100\n",
    "\n",
    "    for i in range(epochs):\n",
    "        if i % 10 == 0:\n",
    "            print('Epochs left', epochs - i)\n",
    "        computed_thetas = back_propagation(loaded_data_x, computed_thetas, learning_rate) \n",
    "        \n",
    "    return computed_thetas\n",
    "\n",
    "cool_thetas = run()\n",
    "\n",
    "[check_network([loaded_theta_one, loaded_theta_two], cool_thetas, loaded_data_x[j * 500]) for j in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
