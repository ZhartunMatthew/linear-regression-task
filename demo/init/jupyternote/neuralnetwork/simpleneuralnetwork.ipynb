{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    input_file = open(path)\n",
    "    output_data = []\n",
    "    for line in input_file:\n",
    "        one_row = line.split(',')\n",
    "        output_data.append([float(x) for x in one_row])\n",
    "\n",
    "    print('Input data info.')\n",
    "    print('\\tFeatures amount: %d, set size %d' % (len(output_data[0]), len(output_data)))\n",
    "    return np.array(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(output_vector):\n",
    "    index_of_max = np.argmax(np.roll(output_vector[1:], 1))\n",
    "    return index_of_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of all pairs theta[i]*input[i] in one neuron\n",
    "# z(i) = sum(theta * x) = theta' * x\n",
    "def unit_summarization(theta, input_vec):\n",
    "    return (np.array(theta) * np.array(input_vec)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the activation function of neuron\n",
    "# a(i) = g(z(i))\n",
    "def unit_activation(theta, input_vec):\n",
    "    sum_result = unit_summarization(theta, input_vec)\n",
    "    return sigmoid(sum_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the activation vector of whole network layer\n",
    "# A = g(z(i)) for all units on layer\n",
    "# [1] - additional neuron (bias)\n",
    "def forward_propagation_step(theta_matrix, input_vec):\n",
    "    activations_vector = [unit_activation(theta_matrix[i], input_vec) for i in range(len(theta_matrix))] \n",
    "    return np.concatenate(([1], activations_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(theta_one, theta_two, input_vec):\n",
    "    # computing input level activation vector\n",
    "    activation_vector_input_layer = np.concatenate(([1], input_vec))\n",
    "    # computing second level activation vector\n",
    "    activation_vector_hidden_layer = forward_propagation_step(theta_one, activation_vector_input_layer)\n",
    "    # computing output level activation vector\n",
    "    activation_vector_output_layer = forward_propagation_step(theta_two, activation_vector_hidden_layer)\n",
    "    return [activation_vector_hidden_layer, activation_vector_output_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regularize_coefficient(lm, m, theta):\n",
    "    coefficient = lm / (2.0 * m)\n",
    "    a = np.array(theta[0]) * np.array(theta[0])\n",
    "    b = np.array(theta[1]) * np.array(theta[1])\n",
    "    return (a.sum() + b.sum()) * coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(x_data, y_data, theta, lm):\n",
    "    temp_sum = 0.0\n",
    "    coefficient = -1 / len(x_data)\n",
    "    fake_zero = 10 ** (-9)\n",
    "    \n",
    "    regularize_coefficient = compute_regularize_coefficient(lm, len(x_data), theta)\n",
    "    for k in range(10):\n",
    "        for i in range(len(x_data)):\n",
    "            hyp_vector = forward_propagation(theta[0], theta[1], x_data[i])[1]\n",
    "            hyp = np.roll(hyp_vector[1:], 1)[k]\n",
    "            y = 1.0 if k * 500 <= i < k * 500 + 500 else 0.0\n",
    "            temp_sum += y * np.log(hyp if hyp != 0.0 else fake_zero) \\\n",
    "                + (1.0 - y) * np.log(1 - hyp if 1 - hyp != 0.0 else fake_zero)\n",
    " \n",
    "    return temp_sum * coefficient + regularize_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_error(theta, next_layer_error, layer_activation):\n",
    "    return (theta.transpose() @ next_layer_error) * layer_activation * (1 - layer_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_delta(prev_delta, layer_error, layer_activation):\n",
    "    return prev_delta + layer_error @ layer_activation.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_partial_derivative(delta, theta, m, lm):\n",
    "    derivative = np.zeros((len(delta), len(delta[0])))\n",
    "    coefficient = 1.0 / m \n",
    "    for i in range(len(delta)):\n",
    "        for j in range(len(delta[0])):\n",
    "            x = lm * theta[i][j]\n",
    "            derivative[i][j] = delta[i][j] + x if j != 0 else delta[i][j]\n",
    "            derivative[i][j] *= coefficient\n",
    "            \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the partial derivative for cost function J using back propagation algorithm\n",
    "def back_propagation(data_x, data_y, thetas=None):\n",
    "    features_amount = 400\n",
    "    hidden_layer_neuron_amount = 25\n",
    "    hidden_layer_inputs_per_neuron = features_amount + 1\n",
    "    output_layer_neuron_amount = 10\n",
    "    output_layer_inputs_per_neuron = hidden_layer_neuron_amount + 1\n",
    "    \n",
    "    # initializing thetas (weights of each connection between neurons)\n",
    "    # weights between input and hidden layer\n",
    "    theta_hidden_layer = np.random.rand(hidden_layer_neuron_amount, hidden_layer_inputs_per_neuron)\n",
    "    # weights between hidden and output layer\n",
    "    theta_output_layer = np.random.rand(output_layer_neuron_amount, output_layer_inputs_per_neuron)\n",
    "    \n",
    "    if thetas is not None:\n",
    "        theta_hidden_layer = thetas[0]\n",
    "        theta_output_layer = thetas[1]\n",
    "    \n",
    "    # deltas matrix, will be used for accumulate partial derivative\n",
    "    hidden_layer_delta = np.zeros((hidden_layer_neuron_amount, hidden_layer_inputs_per_neuron))\n",
    "    output_layer_delta = np.zeros((output_layer_neuron_amount, output_layer_inputs_per_neuron))\n",
    "    \n",
    "    for i in range(len(data_x)):\n",
    "        activation_vectors = forward_propagation(theta_hidden_layer, theta_output_layer, data_x[0])\n",
    "        output_layer_activation = activation_vectors[1]\n",
    "        hidden_layer_activation = activation_vectors[0]\n",
    "        \n",
    "        output_layer_errors = output_layer_activation - data_y[i]\n",
    "        hidden_layer_errors = compute_layer_error(theta_output_layer, output_layer_errors[1:], hidden_layer_activation)\n",
    "        \n",
    "        output_layer_delta = compute_delta(output_layer_delta, output_layer_errors, output_layer_activation)\n",
    "        hidden_layer_delta = compute_delta(hidden_layer_delta, hidden_layer_errors, hidden_layer_activation)\n",
    "        \n",
    "    lm = 1\n",
    "    output_layer_derivative = compute_partial_derivative(output_layer_delta, theta_output_layer, len(data_x), lm)\n",
    "    hidden_layer_derivative = compute_partial_derivative(hidden_layer_delta, theta_hidden_layer, len(data_x), lm)\n",
    "        \n",
    "    return [hidden_layer_derivative, output_layer_derivative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data info.\n\tFeatures amount: 400, set size 5000\nInput data info.\n\tFeatures amount: 1, set size 5000\nInput data info.\n\tFeatures amount: 401, set size 25\nInput data info.\n\tFeatures amount: 26, set size 10\n"
     ]
    }
   ],
   "source": [
    "loaded_data_x = load_data(abspath('demo/init/coursera/neuralnetwork/data/data_x.csv'))\n",
    "loaded_data_y = load_data(abspath('demo/init/coursera/neuralnetwork/data/data_y.csv'))\n",
    "loaded_theta_one = load_data(abspath('demo/init/coursera/neuralnetwork/data/theta1.csv'))\n",
    "loaded_theta_two = load_data(abspath('demo/init/coursera/neuralnetwork/data/theta2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = back_propagation(loaded_data_x, loaded_data_y, [loaded_theta_one, loaded_theta_two])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38448779624289336"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost_function(loaded_data_x, loaded_data_y, [loaded_theta_one, loaded_theta_two], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}