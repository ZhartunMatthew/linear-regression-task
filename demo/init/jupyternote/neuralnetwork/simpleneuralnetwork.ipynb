{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    input_file = open(path)\n",
    "    output_data = []\n",
    "    for line in input_file:\n",
    "        one_row = line.split(',')\n",
    "        output_data.append([float(x) for x in one_row])\n",
    "\n",
    "    print('Input data info.')\n",
    "    print('\\tFeatures amount: %d, set size %d' % (len(output_data[0]), len(output_data)))\n",
    "    return np.array(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(output_vector):\n",
    "    index_of_max = np.argmax(np.roll(output_vector[1:], 1))\n",
    "    return index_of_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of all pairs theta[i]*input[i] in one neuron\n",
    "# z(i) = sum(theta * x) = theta' * x\n",
    "def unit_summarization(theta, input_vec):\n",
    "    return (np.array(theta) * np.array(input_vec)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the activation function of neuron\n",
    "# a(i) = g(z(i))\n",
    "def unit_activation(theta, input_vec):\n",
    "    sum_result = unit_summarization(theta, input_vec)\n",
    "    return sigmoid(sum_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the activation vector of whole network layer\n",
    "# A = g(z(i)) for all units on layer\n",
    "# [1] - additional neuron (bias)\n",
    "def forward_propagation_step(theta_matrix, input_vec):\n",
    "    activations_vector = [unit_activation(theta_matrix[i], input_vec) for i in range(len(theta_matrix))] \n",
    "    return np.concatenate(([1.0], activations_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(theta_one, theta_two, input_vec):\n",
    "    # computing input level activation vector\n",
    "    activation_vector_input_layer = np.concatenate(([1.0], input_vec))\n",
    "    # computing second level activation vector\n",
    "    activation_vector_hidden_layer = forward_propagation_step(theta_one, activation_vector_input_layer)\n",
    "    # computing output level activation vector\n",
    "    activation_vector_output_layer = forward_propagation_step(theta_two, activation_vector_hidden_layer)\n",
    "    return [activation_vector_hidden_layer, activation_vector_output_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regularize_coefficient(lm, m, theta):\n",
    "    coefficient = lm / (2.0 * m)\n",
    "    a = np.array(theta[0]) * np.array(theta[0])\n",
    "    b = np.array(theta[1]) * np.array(theta[1])\n",
    "    return (a.sum() + b.sum()) * coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(i, k):\n",
    "    offset = 500\n",
    "    return 1.0 if k * offset <= i < k * offset + offset else 0.0\n",
    "\n",
    "\n",
    "def get_output_vector(num):\n",
    "    out_vec = [0.0] * 10\n",
    "    out_vec[int(num - 1)] = 1.0\n",
    "    return [1.0] + out_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(x_data, theta, lm):\n",
    "    temp_sum = 0.0\n",
    "    coefficient = -1 / len(x_data)\n",
    "    fake_zero = 10 ** (-9)\n",
    "    \n",
    "    regularize_coefficient = compute_regularize_coefficient(lm, len(x_data), theta)\n",
    "    for k in range(10):\n",
    "        for i in range(len(x_data)):\n",
    "            hyp_vector = forward_propagation(theta[0], theta[1], x_data[i])[1]\n",
    "            hyp = np.roll(hyp_vector[1:], 1)[k]\n",
    "            temp_sum += get_y(i, k) * np.log(hyp if hyp != 0.0 else fake_zero) \\\n",
    "                + (1.0 - get_y(i, k)) * np.log(1 - hyp if 1 - hyp != 0.0 else fake_zero)\n",
    " \n",
    "    return temp_sum * coefficient + regularize_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unit_error(theta, delta):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all layers except last one\n",
    "def compute_layer_error(layer_delta, theta):\n",
    "    return theta.transpose() @ layer_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_delta(layer_error, layer_activation):\n",
    "    layer_error = np.array(layer_error)\n",
    "    return (layer_error * layer_activation * (1.0 - layer_activation))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_theta_correction(prev_layer_activation, delta, rate):\n",
    "    return prev_layer_activation * delta * rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_theta_correction(prev_layer_activation, layer_delta, lr, add_neuron=False):\n",
    "    prev_layer_activation = np.concatenate(([[1.0], prev_layer_activation])) if add_neuron else prev_layer_activation\n",
    "    return [unit_theta_correction(prev_layer_activation, layer_delta[i], lr) for i in range(len(layer_delta))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(x_data, theta, learning_rate):\n",
    "    theta_one = np.array(theta[0])\n",
    "    theta_two = np.array(theta[1])\n",
    "\n",
    "    for i in range(len(x_data)):\n",
    "        # running forward propagation for getting hypothesis  \n",
    "        layer_activations = forward_propagation(theta_one, theta_two, x_data[i])\n",
    "\n",
    "        # getting outputs of hidden and output layer\n",
    "        output_layer_activation = layer_activations[1]\n",
    "        hidden_layer_activation = layer_activations[0]\n",
    "\n",
    "        ############### output layer section ############\n",
    "        # computing output layer error, it's different for all other layers \n",
    "        output_layer_error = output_layer_activation - get_output_vector(i / 500)\n",
    "        # computing deltas, it will be used for computing new theta (new weights)\n",
    "        # delta = error * sigmoid(x)dx\n",
    "        output_layer_delta = compute_layer_delta(output_layer_error, output_layer_activation)\n",
    "        # correcting theta\n",
    "        theta_two -= np.array(layer_theta_correction(hidden_layer_activation, output_layer_delta, learning_rate))\n",
    "\n",
    "        # ############### hidden layer section ############\n",
    "        hidden_layer_error = compute_layer_error(output_layer_delta, theta_two)\n",
    "        hidden_layer_delta = compute_layer_delta(hidden_layer_error, hidden_layer_activation)\n",
    "        theta_one -= np.array(layer_theta_correction(x_data[i], hidden_layer_delta, learning_rate, True))\n",
    "        \n",
    "    return [theta_one, theta_two]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data info.\n\tFeatures amount: 400, set size 5000\nInput data info.\n\tFeatures amount: 1, set size 5000\nInput data info.\n\tFeatures amount: 401, set size 25\nInput data info.\n\tFeatures amount: 26, set size 10\n"
     ]
    }
   ],
   "source": [
    "loaded_data_x = load_data(abspath('demo/init/coursera/neuralnetwork/data/data_x.csv'))\n",
    "loaded_data_y = load_data(abspath('demo/init/coursera/neuralnetwork/data/data_y.csv'))\n",
    "loaded_theta_one = load_data(abspath('demo/init/coursera/neuralnetwork/data/theta1.csv'))\n",
    "loaded_theta_two = load_data(abspath('demo/init/coursera/neuralnetwork/data/theta2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_network(expected_theta, actual_theta, x_data):\n",
    "    expected = forward_propagation(expected_theta[0], expected_theta[1], x_data)[1]\n",
    "    actual = forward_propagation(actual_theta[0], actual_theta[1], x_data)[1]\n",
    "    print('Expected: %d  Actual: %d' % (show_prediction(expected), show_prediction(actual)))\n",
    "    print('Output: ', list(actual[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs left 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 0  Actual: 1\nOutput:  [  9.86827728e-01   9.60466566e-04   7.90068648e-04   4.54337332e-07\n   5.85067841e-03   1.26910517e-03   1.40480399e-03   5.68632203e-05\n   2.52586035e-03   1.00130761e-03]\nExpected: 1  Actual: 1\nOutput:  [  9.23534904e-01   7.71012092e-02   3.68133633e-03   3.22595753e-04\n   1.09813074e-03   1.63443507e-03   4.13746980e-03   1.09429526e-02\n   2.90433429e-04   3.59163879e-03]\nExpected: 2  Actual: 2\nOutput:  [  2.37963051e-04   8.52696956e-01   2.03403673e-04   8.49851518e-05\n   6.35927850e-03   1.70590425e-04   1.38252695e-04   2.69786717e-01\n   4.25687553e-03   1.70053593e-03]\nExpected: 3  Actual: 3\nOutput:  [  6.21395702e-03   5.99368108e-04   9.90101975e-01   1.64850914e-04\n   5.79272659e-03   2.55010992e-06   1.18093531e-03   5.49118699e-02\n   2.69313162e-02   2.08443358e-03]\nExpected: 4  Actual: 4\nOutput:  [  4.14678493e-04   2.13135222e-02   8.82159198e-04   9.81866947e-01\n   1.39708701e-04   2.60858187e-03   5.26992834e-02   2.08824141e-03\n   1.96227345e-04   3.70856935e-03]\nExpected: 5  Actual: 5\nOutput:  [  4.12982015e-03   4.98157961e-04   3.59492675e-02   5.95991364e-07\n   9.06429846e-01   3.59662344e-05   2.40186374e-03   4.26192861e-02\n   2.34575355e-03   1.40345356e-03]\nExpected: 6  Actual: 6\nOutput:  [  5.79551547e-03   1.04979299e-02   3.89415933e-06   3.90533804e-03\n   1.68135678e-03   9.84829969e-01   4.86253039e-06   6.88045732e-03\n   7.78405444e-05   1.61319810e-03]\nExpected: 7  Actual: 7\nOutput:  [  2.08887999e-03   1.83587573e-05   2.61562664e-05   2.49285766e-03\n   1.04006997e-03   2.93076468e-06   7.69948964e-01   2.17114401e-05\n   4.95017640e-01   8.71935004e-04]\nExpected: 8  Actual: 8\nOutput:  [  1.74951137e-02   7.36416637e-03   7.91231216e-04   1.96139356e-04\n   2.71789586e-02   5.09758934e-05   5.56029699e-06   9.80407102e-01\n   4.58896083e-01   2.11580613e-03]\nExpected: 9  Actual: 9\nOutput:  [  2.97551004e-02   1.40074478e-04   3.93104447e-05   5.85617716e-02\n   4.09388963e-04   6.24601614e-05   6.04293173e-03   4.16362377e-04\n   9.85851762e-01   1.92635179e-03]\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    features_amount = 400\n",
    "    hidden_layer_neuron_amount = 25\n",
    "    hidden_layer_inputs_per_neuron = features_amount + 1\n",
    "    output_layer_neuron_amount = 10\n",
    "    output_layer_inputs_per_neuron = hidden_layer_neuron_amount + 1\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    epochs = 200\n",
    "    \n",
    "    computed_thetas = [[], []]\n",
    "    # initializing thetas (weights of each connection between neurons)\n",
    "    # weights between input and hidden layer\n",
    "    computed_thetas[0] = np.random.rand(hidden_layer_neuron_amount, hidden_layer_inputs_per_neuron) / 100\n",
    "    # weights between hidden and output layer\n",
    "    computed_thetas[1] = np.random.rand(output_layer_neuron_amount, output_layer_inputs_per_neuron) / 100\n",
    "\n",
    "    for i in range(epochs):\n",
    "        if i % 10 == 0:\n",
    "            print('Epochs left', epochs - i)\n",
    "            \n",
    "        computed_thetas = back_propagation(loaded_data_x, computed_thetas, learning_rate) \n",
    "        \n",
    "    return computed_thetas\n",
    "\n",
    "cool_thetas = run()\n",
    "\n",
    "for i in range(10):\n",
    "    check_network([loaded_theta_one, loaded_theta_two], cool_thetas, loaded_data_x[i * 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
